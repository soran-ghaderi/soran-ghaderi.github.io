---
layout: page
title: Skills And Expertise
permalink: /skills
comments: false
---


<div class="row justify-content-between">
<div class="col-md-8 pr-5">
    <p>I've engaged in a diverse array of machine learning projects, spanning areas such
        as computer vision, natural language processing, and generative modeling. These experiences have allowed me to
        refine my skills across various tools and technologies, including TensorFlow and PyTorch. They've
        enabled me to approach problems with creativity and a commitment to robust solutions.
        </p>

    <h3>Machine learning</h3>
    <h4>Computer vision</h4>
    <ul>
        <li><b>YOLO 3 for car detection and object localization</b>, Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - You
            Only Look Once: Unified, Real-Time Object Detection (2015), <a href="">#autonomous_driving</a></li>
        <li><b>Resnet50 for hand sign recognition</b>, Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed,
            S., Anguelov, D., Erhan, D., Vanhoucke, V., & Rabinovich, A. (2014). Going Deeper with Convolutions.,
            Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - Deep Residual Learning for Image Recognition (2015)</li>
        <li><b>Transfer learning and fine-tuning pretrained models for car recognition</b></li>
        <li><b>Inception network, FaceNet in Face recognition</b>: Reimplementation of "Florian Schroff, Dmitry Kalenichenko, James Philbin (2015).
            FaceNet: A Unified Embedding for Face Recognition and Clustering"</li>
        <li><b>Neural style transfer</b>, "Leon A. Gatys, Alexander S. Ecker,
            Matthias Bethge, (2015). A Neural Algorithm of Artistic Style", <a href="">#generative</a>, </li>
        <li><b>Classifying volcanoes on Venus</b>, trained on volcanoes on Venus dataset (Pytorch)</li>
        <li><b>Generating fruit image using variational autoencoders</b>, <a href="">#generative</a> </li>
        <li><b><a href="https://github.com/soran-ghaderi/make-a-video" target="_blank">
        Make a video</a></b> <i>(partially implemented - in-progress)</i>, Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual,
        O., Gafni, O., Parikh, D., Gupta, S.,& Taigman, Y. (2022). Make-A-Video: Text-to-Video Generation without
        Text-Video Data. <a href="">#generative</a> </li>
    </ul>
    <h4>Natural Language Processing</h4>
    <ul>
        <li><b>Neural machine translation using Attention mechanisms</b></li>
        <li><b>Neural machine translation (fr-to-en) using original Transformer model (Implemented using
            <a href="https://github.com/tensorops/TransformerX" target="_blank">TransformerX</a> library)</b>,
            Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin,
            I. (2017). Attention Is All You Need.</li>
        <li><b>Fine-tuning TF-hub pretrained models for text classification</b> and visualizing metrics using Tensorboard</li>
        <li><b>Character-level text generation</b> using RNNs, <a href="">#generative</a> </li>
        <li><b>Word-level text generation</b> using LSTM to generate poems in the style of Shakespeare, <a href="">#generative</a> </li>
        <li>Trained sequence models using various embedding layers and word vector representations i.e. Word2Vec,
            GloVe word vectors</li>
        <li><b>Sentiment classifier using LSTM and GloVe-6B-50d word vector representations</b> for suggesting most relevant emojis regarding the input text,
            trained on EMOJISET dataset</li>
        <li><b>Trained sequence models using attention mechanisms</b></li>
        <li><b>Tweet emotion recognition</b></li>
    </ul>
    <h4>Speech processing</h4>
    <ul>
        <li><b>Improvise Jazz solo</b> using an LSTM model trained on a corpus of Jazz music, <a href="">#generative</a> </li>
        <li><b>Deep learning for trigger word detection</b></li>
    </ul>
    <h4>Deep learning frameworks</h4>
    <ul>
        <li><b>Tensorflow:</b> Mainly used it in different projects such as python libraries, CV, NLP projects.</li>
        <li><b>Keras:</b> Used it along Tensorflow</li>
        <li><b>Pytorch:</b> Brief experience in implementing a few models</li>
        <li><b></b></li>
    </ul>
    <h4>Programming Languages</h4>
    <ul>
        <li><b>Python:</b> Proficient in developing Python-based solutions for machine learning and deep learning tasks,
            including data pre-processing, feature engineering, and model development and evaluation.</li>
        <li><b>C++:</b> Basic understanding of C++14, with experience using it for implementing computer vision
            algorithms using OpenCV library. Also, Developed various algorithms for data processing and analysis using
            C++98 in university coursework, including sorting, searching, and graph algorithms.</li>
    </ul>
    <h4>Libraries</h4>
    <ul>
        <li><b>Numpy and Pandas:</b> Experience in using NumPy and Pandas for data manipulation, cleaning, and transformation.</li>
        <li><b>Scikit-learn:</b> Experience with Scikit-learn for implementing supervised and unsupervised machine
            learning algorithms for classification, regression, and clustering tasks. Experience mainly gained
            through university coursework and a few standalone projects.
        </li>
        <li><b>Matplotlib:</b> Familiar with Matplotlib for data visualization and plotting.</li>
    </ul>
    <h4>Tools</h4>
    <ul>
        <li><b>NVIDIA TensorRT:</b> Basic understanding of using TensorRT to optimize deep learning models for inference on
            NVIDIA GPUs.</li>
        <li><b>CUDA:</b> Basic understanding of using CUDA for parallel computing and accelerating deep learning
            algorithms.</li>
        <li><b>Note:</b> Experience gained through different courses and a few personal projects.</li>
    </ul>
    <h3>Software engineering</h3>
    <ul>
        <li>Developed multiple open-source Python libraries on GitHub and deployed them on PyPI, including TransformerX,
            Emgraph, and Bigraph. These libraries are actively downloaded and being used by users.</li>
        <li>Designed and implemented the architecture for the libraries using best practices such as object-oriented
            programming, modular design, and version control with Git.</li>
        <li>Collaborated with other developers on GitHub to contribute to open-source projects and perform code
            reviews for pull requests.</li>
        <li>Developed a Telegram music search engine named TASE using Python, integrating various APIs and technologies
            such as Elasticsearch, Pyrogram, and ArangoDB. Implemented a scalable and fault-tolerant architecture
            using RabbitMQ and Celery.</li>
        <li>Utilized agile development methodologies such as Kanban to manage project tasks and ensure timely delivery
            of features.</li>
        <li>Developed documentation and test cases for the libraries and the search engine, ensuring high code quality
            and maintainability.</li>
        <li>Developed high-performance web applications using asynchronous and multiprocessing programming techniques in
            Python, leveraging libraries such as `multiprocessing`, `threading`, `ascyncio`, and `concurrent.futures`.</li>
    </ul>

    <h3>Technical Writing and AI Research Blog</h3>
    <ul>
        <li><b>Platforms:</b> Medium and Towards Data Science</li>
        <li><b>Articles:</b> Authored and published articles on attention mechanisms and Transformers:
            <ul>
                <li><a href="https://towardsdatascience.com/the-map-of-transformers-e14952226398">
                    The Map Of Transformers</a></li>
                <li>
                    <a href="https://towardsdatascience.com/transformers-in-action-attention-is-all-you-need-ac10338a023a">
                        Transformers in Action: Attention Is All You Need</a></li>
                <li><a href="https://towardsdatascience.com/rethinking-thinking-how-do-attention-mechanisms-actually-work-a6f67d313f99">
                    Rethinking Thinking: How Do Attention Mechanisms Actually Work?</a></li>
            </ul>
        </li>
        <li><b>Notes:</b> Conducted research and synthesized complex technical concepts into clear and accessible content for a broad audience.</li>
    </ul>
    <h3>Languages and Interests</h3>
    <ul>
        <li><b>Languages:</b> Fluent in English, Kurdish, and Persian.</li>
        <li><b>Interests:</b> I enjoy staying up-to-date on the latest developments in the field of artificial
            intelligence, and regularly read outstanding papers in the field. In my free time, I enjoy strolling around
            the city or hiking.</li>
    </ul>

    <!--    <p class="mb-5"><img class="shadow-lg" src="{{site.url}}/assets/images/mediumish-jekyll-template.png" alt="jekyll template mediumish" /></p>-->

<!--    <p>This website is built with Jekyll and Mediumish template for Jekyll. It's for demonstration purposes, no real content can be found. Mediumish template for Jekyll is compatible with Github pages, in fact even this demo is created with Github Pages and hosted with Github.</p>-->


<!--<h4>Questions or bug reports?</h4>-->

<!--<p>Head over to our <a href="https://github.com/wowthemesnet/mediumish-theme-jekyll">Github repository</a>!</p>-->

</div>

<!--<div class="col-md-4">-->

<!--<div class="sticky-top sticky-top-80">-->
<!--<h5>Buy me a coffee</h5>-->

<!--<p>Thank you for your support! Your donation helps me to maintain and improve <a target="_blank" href="https://github.com/wowthemesnet/mediumish-theme-jekyll">Mediumish <i class="fab fa-github"></i></a>.</p>-->

<!--<a target="_blank" href="https://www.wowthemes.net/donate/" class="btn btn-danger">Buy me a coffee</a> <a target="_blank" href="https://bootstrapstarter.com/bootstrap-templates/template-mediumish-bootstrap-jekyll/" class="btn btn-warning">Documentation</a>-->

<!--</div>-->
<!--</div>-->
</div>
