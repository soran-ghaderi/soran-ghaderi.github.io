---
layout: post
title:  NIR for Code Generation
author: Soran Ghaderi

[//]: # (categories: [ library, python, machine learning, deep learning, energy-based models ])

image: images/nir.png
featured: true
hidden: false
comments: true
excerpt: üí° Separating reasoning from generation in LLMs for coding, then reintegrating hidden states from the thinking stage directly into the main generation at various layers. 
---


<h2 align="center"> <b><a target="_blank" href="https://soran-ghaderi.github.io/nir/">Neural Integration of Iterative Reasoning (NIR) in LLMs for Code Generation
</a></b></h2>
 
## Abstract
Despite advances in large language models (LLMs) for code generation, they still struggle in effectively utilizing contextual information throughout the generation process. To tackle this challenge, we introduce the Neural Integration of Iterative Reasoning (NIR) framework, which offers a new method for incorporating Context Representation Vectors (CRVs) at multiple levels within LLMs. NIR boosts the ability of these models to generate code without needing fine-tuning, allowing it to be used across various LLM architectures. We assess NIR by testing it with LLaMA 3.1 on the MBPP dataset, focusing on early, mid, and deep integration stages. Our experiments show that the depth of CRV integration has a notable impact on several facets of code generation, including response rates, syntactic correctness, and overall code structure. Deeper integration generally improves syntactic accuracy and code conciseness, while mid-layer integration shows optimal performance in semantic tasks. We report detailed evaluation metrics that assess code quality, complexity, and structure. Our findings indicate possible trade-offs among various code quality measures and emphasize the potential of adaptive integration strategies. While NIR demonstrates promising results, we also identify limitations such as dataset specificity and output inconsistencies. This study contributes to understanding contextual information processing in LLMs and might be useful for future developments in codeLLMs. We conclude by outlining future research directions, including multi-layer integration and dynamic adaptation strategies.



[//]: # (<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/soran-ghaderi/torchebm?style=social">)
[//]: # (<img alt="PyPI - Downloads" src="https://img.shields.io/pypi/dm/torchebm">)
[//]: # (<img src="https://img.shields.io/pypi/v/torchebm.svg" alt="latest release" />)

## Highlights:

‚úÖ It dynamically recalculates RoPE for modified hidden states and requires NO finetuning - a plug-and-play approach to enhance code generation capabilities!

‚úÖ Using Meta's LLaMA 3.1-8B-instruct as our testbed, we found multiple trade-offs in code characteristics depending on where integration happens.

‚úÖ A key insight: LLMs encode simpler code features in early layers and more sophisticated patterns in later ones. -> This can be helpful for targeted manipulation

‚úÖ The method is flexible - we can integrate arbitrary hidden states into arbitrary layers (though neighboring layers work best for model coherence).
#Reasoning  #MachineLearning  #GenerativeAI

Interested in the [full research](https://soran-ghaderi.github.io/nir/nir.pdf)? Please visit [NIR website](http://soran-ghaderi.github.io/nir/) to explore methodology, results, and [code implementation](https://github.com/soran-ghaderi/nir_code_release).

If you'd like to discuss this further, please don't hesitate to reach out to me.

Please check out the following twitter threads üßµ : <br> <br>


<table class="twitter-table">
  <tr>
    <td>
      <blockquote class="twitter-tweet"><p lang="en" dir="ltr">A Walkthrough of the architecture from my Master's thesis &quot;Neural Integration of Iterative Reasoning (NIR) in LLMs for Code Generation&quot;. <a href="https://t.co/4wZEVjp0FF">pic.twitter.com/4wZEVjp0FF</a></p>&mdash; Soran Ghaderi (@soranghadri) <a href="https://twitter.com/soranghadri/status/1892032650266738810?ref_src=twsrc%5Etfw">February 19, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    </td>
    <td>
      <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excited to share my Master&#39;s thesis &quot;Neural Integration of Iterative Reasoning (NIR) in LLMs for Code Generation&quot;! <br><br>idea: separating reasoning from generation in LLMs for coding, then reintegrating hidden states from the thinking stage directly into the main generation üßµ1/n <a href="https://t.co/oEqCKrBcOY">pic.twitter.com/oEqCKrBcOY</a></p>&mdash; Soran Ghaderi (@soranghadri) <a href="https://twitter.com/soranghadri/status/1891959752357191763?ref_src=twsrc%5Etfw">February 18, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    </td>
  </tr>
</table>

## Collaboration
I'm interested in handling OOD problems and those mentioned on my [website](http://soran-ghaderi.github.io) üåê, if you like to collaborate, please reach out to me.<br><br>

and let's connect! üî¨<br>
üê¶ [Twitter](https://x.com/soranghadri) <br>
üíª [GitHub](http://github.com/soran-ghaderi) <br>
üîó [LinkedIn](http://linkedin.com/in/soran-ghaderi) <br>

<style>
  .twitter-table {
    width: 100%;
    max-width: 1200px; /* Adjust as needed */
    margin: auto; /* Center the table */
    border-collapse: collapse; /* Remove spacing between cells */
  }

.twitter-table td {
width: 50%; /* Each cell takes up half the table width */
padding: 10px;
border: 0px solid #ddd;
box-sizing: border-box; /* Include padding and border in the element's total width and height */
}
</style>    
